{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point position slope heading way english line course while back\n",
      "point position slope heading way english line course while back\n",
      "point position slope heading way english line course while back\n",
      "point position slope heading way english line course while back\n",
      "point position slope heading way english line course while back\n",
      "point position slope heading way english line course while back\n",
      "point position slope heading way english line course while back\n",
      "point position slope heading way english line course while back\n",
      "point position slope heading way english line course while back\n",
      "point position slope heading way english line course while back\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit.magnitude'), topn=10)\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split(), similarity_measure=0)))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=47.0933646506\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.10f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the retrofitted .magnitude file, run this command:\n",
    "\n",
    "`sh run.sh`\n",
    "\n",
    "This script reads the original word vector file `glove.6B.100d.magnitude` from the given CSIL path. \n",
    "\n",
    "It will take approximatelty 15 minutes to complete the process. The script runs `modifyWordVec.py` file that generates the retrofiited word vectors in a text file. It is by default reading the `wordnet-synonyms.txt` for reading the lexicon to create ontology graph. The final step generates '.magnitude' file from the generated '.txt' file. \n",
    "\n",
    "\n",
    "The `modifyWordVec.py` file first reads the pymagnitude word vectors `Q̂`.\n",
    "Then create a new copy of it `Q`.\n",
    "Then for 10 iterations it loops through each word vector from the ontology lexicon file and for each lexicon word vector it modifies the vectors in `Q` such that the vectors stay close to `Q̂` and at the same time the adjacent vectors from the ontology file come close to each other in `Q`. \n",
    "\n",
    "We minimize the $$L(Q)=\\sum_{i=1}^n[α_i||q_i−q̂_i||^2+\\sum_{(i,j)∈E}^{ }β_{ij}||q_i−q_j||^2]$$\n",
    "\n",
    "Overall the algorithm looks like this:\n",
    "    \n",
    "<br>1. Initialize Q to be equal to the vectors in Q̂\n",
    "<br>2. For iterations t= 1 … 10\n",
    "&nbsp;&nbsp;&nbsp;Take the derivative of L(Q) wrt each qi word vector and assign it to zero to get an update:\n",
    "    &nbsp;$$q_i=\\sum_{j:(i,j)∈E}^{ }β_{ij}q_j+α_iq̂_i\\sum_{j:(i,j)∈E}^{ }β_{ij}+α_i$$\n",
    "    \n",
    "\n",
    "We have created 4 functions `add_similarity`, `baladd_similarity`, `mul_similarity` and `balmul_similarity` to calculate the 4 substitutability measures from the reference paper\n",
    "\n",
    "`add_similarity` uses the following similiarity calculation to get the similarity value between a candidate target word and the context words\n",
    "$$\\frac{\\cos(s,t)+\\sum_{c∈C}^{ }\\cos(s,c)}{|C|+1}$$\n",
    "\n",
    "`baladd_similarity` uses the following similiarity calculation to get the similarity value between a candidate target word and the context words\n",
    "$$\\frac{|C|·\\cos(s,t)+\\sum_{c∈C}^{ }\\cos(s,c)}{2·|C|}$$\n",
    "\n",
    "`mul_similarity` uses the following similiarity calculation to get the similarity value between a candidate target word and the context words\n",
    "$$\\sqrt[|C|+1]{p\\cos(s,t)·\\prod_{c∈C}^{ }p\\cos(s,c)}$$\n",
    "\n",
    "`balmul_similarity` uses the following similiarity calculation to get the similarity value between a candidate target word and the context words\n",
    "$$\\sqrt[2·|C|]{p\\cos(s,t)^{|C|}·\\prod_{c∈C}^{ }p\\cos(s,c)}$$\n",
    "\n",
    "We have copy pasted stop words from NLTK corpus and also added a few words we wanted to ignore like `\"e.g.\", \"http://pgina.xpasystems.com\",\"'s\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - with stop words and no context words using `wordnet-synonyms.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=47.0933646506\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit.magnitude'), topn=10)\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split(), similarity_measure=0)))\n",
    "\n",
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.10f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With context words - Add and stop words and considering top 20 words using `wordnet-synonyms.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=38.1092190252\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit.magnitude'), topn=20)\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split(), similarity_measure=1)))\n",
    "\n",
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.10f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With context words - BalAdd and stop words and considering top 20 words using `wordnet-synonyms.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=45.9189665297\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit.magnitude'), topn=20)\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split(), similarity_measure=2)))\n",
    "\n",
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.10f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With context words - Mul and stop words and considering top 20 words using `wordnet-synonyms.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=47.0933646506\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit.magnitude'), topn=20)\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split(), similarity_measure=3)))\n",
    "\n",
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.10f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With context words - BalMul and stop words and considering top 20 words using `wordnet-synonyms.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=47.0933646506\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit.magnitude'), topn=20)\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split(), similarity_measure=4)))\n",
    "\n",
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.10f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrofitting: ###\n",
    "\n",
    "We were getting a dev score of `45.04` with retrofitting using the `wordnet-synonyms.txt` lexicon. \n",
    "\n",
    "### Tuning alpha and beta: ###\n",
    "\n",
    "We managed to get a higher dev score of `47.0933646506` by iteratively changing the alpha and with beta = (degree of node)^-1.\n",
    "\n",
    "### Incorporating context words without stop words: ###\n",
    "\n",
    "On top of tuning the parameters, we also incorporated the context words (not including the stop words) to improve the dev score. We tried different methods for this:\n",
    "\n",
    "1. We tried to calculate the average word vector that takes the mean effect of the target word and the context words chosen in a particular context window. However, this approach resulted in a lower score of `38.5413` .\n",
    "\n",
    "\n",
    "2. As given in the reference paper, we calculated the 4 substitutability measures based on Add, BalAdd, Mult and BalMult measuring methods. This gave us no improvement on the base model since this measure considers context vectors not the word vectors of the context words.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
